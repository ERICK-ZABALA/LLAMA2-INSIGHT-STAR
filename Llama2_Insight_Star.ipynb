{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1R-rbSIqje_VoXYSy7uton-sNJacc3HRZ",
      "authorship_tag": "ABX9TyMe08nIDOxRi6jiMwDU98dU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ERICK-ZABALA/LLAMA2-INSIGHT-STAR/blob/main/Llama2_Insight_Star.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install colab_ssh on google colab\n",
        "!pip install colab_ssh --upgrade --quiet\n",
        "from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n",
        "ssh_tunnel_password = \"C1sc0123!\" #@param {type:\"string\"}\n",
        "launch_ssh_cloudflared(password=ssh_tunnel_password)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "7epGGj6_U9OI",
        "outputId": "55dce049-b583-4fa7-eb0a-ebf5761476df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "*{\n",
              "\toutline:none;\n",
              "}\n",
              "code{\n",
              "\tdisplay:inline-block;\n",
              "\tpadding:5px 10px;\n",
              "\tbackground: #444;\n",
              "\tborder-radius: 4px;\n",
              "\twhite-space: pre-wrap;\n",
              "\tposition:relative;\n",
              "\tcolor:white;\n",
              "}\n",
              ".copy-code-button{\n",
              "\tfloat:right;\n",
              "\tbackground:#333;\n",
              "\tcolor:white;\n",
              "\tborder: none;\n",
              "\tmargin: 0 0 0 10px;\n",
              "\tcursor: pointer;\n",
              "}\n",
              "p, li{\n",
              "\tmax-width:700px;\n",
              "}\n",
              ".choices{\n",
              "\tdisplay:flex;\n",
              "\tflex: 1 0 auto;\n",
              "}\n",
              ".choice-section{\n",
              "\tborder:solid 1px #555;\n",
              "\tborder-radius: 4px;\n",
              "\tmin-width:300px;\n",
              "\tmargin: 10px 15px 0 0;\n",
              "\tpadding: 0 15px 15px 15px ;\n",
              "}\n",
              ".button{\n",
              "\tpadding: 10px 15px;\n",
              "\tbackground:#333;\n",
              "\tborder-radius: 4px;\n",
              "\tborder:solid 1px #555;\n",
              "\tcolor:white;\n",
              "\tfont-weight:bold;\n",
              "\tcursor:pointer;\n",
              "}\n",
              ".pill{\n",
              "\tpadding:2px 4px;\n",
              "\tborder-radius: 100px;\n",
              "\tbackground-color:#e65858;\n",
              "\tfont-size:12px;\n",
              "\tfont-weight:bold;\n",
              "\tmargin: 0 15px;\n",
              "\tcolor:white;\n",
              "}\n",
              "</style>\n",
              "<details class=\"choice-section\">\n",
              "\t<summary style=\"cursor:pointer\">\n",
              "\t\t<h3 style=\"display:inline-block;margin-top:15px\">⚙️ Client machine configuration<span class=\"pill\">Required</span></h3>\n",
              "\t</summary>\n",
              "\t<p>Don't worry, you only have to do this <b>once per client machine</b>.</p>\n",
              "\t<ol>\n",
              "\t\t<li>Download <a href=\"https://developers.cloudflare.com/argo-tunnel/getting-started/installation\">Cloudflared (Argo Tunnel)</a>, then copy the absolute path of the cloudflare binary</li>\n",
              "\t\t<li>Now, you have to append the following to your SSH config file (usually under ~/.ssh/config), and make sure you replace the placeholder with the path you copied in Step 1:</li>\n",
              "\t</ol>\n",
              "\t<code>Host *.trycloudflare.com\n",
              "\tHostName %h\n",
              "\tUser root\n",
              "\tPort 22\n",
              "\tProxyCommand &ltPUT_THE_ABSOLUTE_CLOUDFLARE_PATH_HERE&gt access ssh --hostname %h\n",
              "\t</code>\n",
              "</details>\n",
              "<div class=\"choices\">\n",
              "\t<div class=\"choice-section\">\n",
              "\t\t<h4>SSH Terminal</h4>\n",
              "\t\t<p>To connect using your terminal, type this command:</p>\n",
              "\t\t<code>ssh mississippi-readily-elizabeth-newsletter.trycloudflare.com</code>\n",
              "\t</div>\n",
              "\t<div class=\"choice-section\">\n",
              "\t\t<h4>VSCode Remote SSH</h4>\n",
              "\t\t<p>You can also connect with VSCode Remote SSH (Ctrl+Shift+P and type \"Connect to Host...\"). Then, paste the following hostname in the opened command palette:</p>\n",
              "\t\t<code>mississippi-readily-elizabeth-newsletter.trycloudflare.com</code>\n",
              "\t</div>\n",
              "</div>\n",
              "\n",
              "<script>\n",
              "// Copy any string\n",
              "function fallbackCopyTextToClipboard(text) {\n",
              "  var textArea = document.createElement(\"textarea\");\n",
              "  textArea.value = text;\n",
              "  \n",
              "  // Avoid scrolling to bottom\n",
              "  textArea.style.top = \"0\";\n",
              "  textArea.style.left = \"0\";\n",
              "  textArea.style.position = \"fixed\";\n",
              "\n",
              "  document.body.appendChild(textArea);\n",
              "  textArea.focus();\n",
              "  textArea.select();\n",
              "\n",
              "  try {\n",
              "    var successful = document.execCommand('copy');\n",
              "    var msg = successful ? 'successful' : 'unsuccessful';\n",
              "    console.log('Fallback: Copying text command was ' + msg);\n",
              "  } catch (err) {\n",
              "    console.error('Fallback: Oops, unable to copy', err);\n",
              "  }\n",
              "\n",
              "  document.body.removeChild(textArea);\n",
              "}\n",
              "\n",
              "// Show the copy button with every code tag\n",
              "document.querySelectorAll('code').forEach(function (codeBlock) {\n",
              "\tconst codeToCopy= codeBlock.innerText;\n",
              "\tvar pre = document.createElement('pre');\n",
              "\tpre.innerText = codeToCopy;\n",
              "    var button = document.createElement('button');\n",
              "    button.className = 'copy-code-button';\n",
              "    button.type = 'button';\n",
              "    button.innerText = 'Copy';\n",
              "\tbutton.onclick = function(){\n",
              "\t\tfallbackCopyTextToClipboard(codeToCopy);\n",
              "\t\tbutton.innerText = 'Copied'\n",
              "\t\tsetTimeout(()=>{\n",
              "\t\t\tbutton.innerText = 'Copy'\n",
              "\t\t},2000)\n",
              "\t}\n",
              "\tcodeBlock.children = pre;\n",
              "\tcodeBlock.prepend(button)\n",
              "});\n",
              "</script>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc \\\n",
        "\t| sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null \\\n",
        "\t&& echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" \\\n",
        "\t| sudo tee /etc/apt/sources.list.d/ngrok.list \\\n",
        "\t&& sudo apt update \\\n",
        "\t&& sudo apt install ngrok"
      ],
      "metadata": {
        "id": "6lLPUfK6542E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 4DjV1ysYxrX3eY9XDAqpC_6HywAcsNVPCyiYZ7U2knG\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AUP7Alb5aiq",
        "outputId": "a76abe21-e12c-4194-aa1d-a4b74fd2ee52"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ugm0iLw-_p-",
        "outputId": "2cb040a0-7924-4034-b429-244953c32a4b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPj48bp2zEWi",
        "outputId": "3b7258d4-657d-44db-e976-3764eaeb2f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/ollama'\n",
            "/content\n",
            ">>> Downloading ollama...\n",
            "############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "System has not been booted with systemd as init system (PID 1). Can't operate.\n",
            "Failed to connect to bus: Host is down\n"
          ]
        }
      ],
      "source": [
        "# Cambiar al directorio /content/ollama\n",
        "%cd /content/ollama\n",
        "\n",
        "# Ejecutar los comandos\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!command -v systemctl >/dev/null && sudo systemctl stop ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiohttp pyngrok\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "from aiohttp import ClientSession\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library becomes preferred\n",
        "# over the built-in library. This is particularly important for\n",
        "# Google Colab which installs older drivers\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmEZSf9g4Z6s",
        "outputId": "90270997-2a86-418b-8b02-928240be8438"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.9.3)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.6)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiohttp pyngrok\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "from aiohttp import ClientSession\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library becomes preferred\n",
        "# over the built-in library. This is particularly important for\n",
        "# Google Colab which installs older drivers\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "async def run(cmd):\n",
        "  '''\n",
        "  run is a helper function to run subcommands asynchronously.\n",
        "  '''\n",
        "  print('>>> starting', *cmd)\n",
        "  p = await asyncio.subprocess.create_subprocess_exec(\n",
        "      *cmd,\n",
        "      stdout=asyncio.subprocess.PIPE,\n",
        "      stderr=asyncio.subprocess.PIPE,\n",
        "  )\n",
        "\n",
        "  async def pipe(lines):\n",
        "    async for line in lines:\n",
        "      print(line.strip().decode('utf-8'))\n",
        "\n",
        "  await asyncio.gather(\n",
        "      pipe(p.stdout),\n",
        "      pipe(p.stderr),\n",
        "  )\n",
        "\n",
        "\n",
        "await asyncio.gather(\n",
        "    run(['ollama', 'serve']),\n",
        "    run(['ngrok', 'http', '--log', 'stderr', '11434','--host-header=\"localhost:11434\"']),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq7JQCWQJhzT",
        "outputId": "f95cf9cf-d73e-4bc1-a8f9-f1f124a16c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.9.3)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp) (3.6)\n",
            ">>> starting ollama serve\n",
            ">>> starting ngrok http --log stderr 11434 --host-header=\"localhost:11434\"\n",
            "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
            "Your new public key is:\n",
            "\n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIG8N0L9y6KE08OJrjdMaxPlulw5R60TRgoq2JJZ6gXzD\n",
            "\n",
            "time=2024-04-07T03:36:12.523Z level=INFO source=images.go:804 msg=\"total blobs: 0\"\n",
            "time=2024-04-07T03:36:12.523Z level=INFO source=images.go:811 msg=\"total unused blobs removed: 0\"\n",
            "time=2024-04-07T03:36:12.523Z level=INFO source=routes.go:1118 msg=\"Listening on 127.0.0.1:11434 (version 0.1.30)\"\n",
            "time=2024-04-07T03:36:12.527Z level=INFO source=payload_common.go:113 msg=\"Extracting dynamic libraries to /tmp/ollama3521285080/runners ...\"\n",
            "t=2024-04-07T03:36:13+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "t=2024-04-07T03:36:13+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "t=2024-04-07T03:36:13+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "t=2024-04-07T03:36:13+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "t=2024-04-07T03:36:13+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "t=2024-04-07T03:36:13+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2024-04-07T03:36:13+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:11434 url=https://9613-35-243-214-20.ngrok-free.app\n",
            "time=2024-04-07T03:36:21.877Z level=INFO source=payload_common.go:140 msg=\"Dynamic LLM libraries [cpu cpu_avx cuda_v11 cpu_avx2 rocm_v60000]\"\n",
            "time=2024-04-07T03:36:21.877Z level=INFO source=gpu.go:115 msg=\"Detecting GPU type\"\n",
            "time=2024-04-07T03:36:21.877Z level=INFO source=gpu.go:265 msg=\"Searching for GPU management library libcudart.so*\"\n",
            "time=2024-04-07T03:36:21.887Z level=INFO source=gpu.go:311 msg=\"Discovered GPU libraries: [/tmp/ollama3521285080/runners/cuda_v11/libcudart.so.11.0 /usr/local/cuda/lib64/libcudart.so.12.2.140]\"\n",
            "time=2024-04-07T03:36:21.889Z level=INFO source=gpu.go:340 msg=\"Unable to load cudart CUDA management library /tmp/ollama3521285080/runners/cuda_v11/libcudart.so.11.0: cudart init failure: 35\"\n",
            "time=2024-04-07T03:36:21.890Z level=INFO source=gpu.go:340 msg=\"Unable to load cudart CUDA management library /usr/local/cuda/lib64/libcudart.so.12.2.140: cudart init failure: 35\"\n",
            "time=2024-04-07T03:36:21.891Z level=INFO source=gpu.go:265 msg=\"Searching for GPU management library libnvidia-ml.so\"\n",
            "time=2024-04-07T03:36:21.898Z level=INFO source=gpu.go:311 msg=\"Discovered GPU libraries: []\"\n",
            "time=2024-04-07T03:36:21.898Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\n",
            "time=2024-04-07T03:36:21.899Z level=INFO source=routes.go:1141 msg=\"no GPU detected\"\n",
            "t=2024-04-07T03:37:00+0000 lvl=info msg=\"join connections\" obj=join id=753a061ecda3 l=127.0.0.1:11434 r=192.226.134.67:47950\n",
            "[GIN] 2024/04/07 - 03:37:00 | 200 |      84.483µs |  192.226.134.67 | GET      \"/\"\n",
            "[GIN] 2024/04/07 - 03:37:00 | 404 |       8.503µs |  192.226.134.67 | GET      \"/favicon.ico\"\n",
            "t=2024-04-07T03:41:16+0000 lvl=info msg=\"join connections\" obj=join id=bfd68c6fbdba l=127.0.0.1:11434 r=192.226.134.67:47950\n",
            "[GIN] 2024/04/07 - 03:41:16 | 200 |      47.879µs |  192.226.134.67 | GET      \"/\"\n",
            "[GIN] 2024/04/07 - 03:42:05 | 200 |      22.462µs |  192.226.134.67 | GET      \"/\"\n",
            "t=2024-04-07T03:43:05+0000 lvl=info msg=\"join connections\" obj=join id=107838ed3068 l=127.0.0.1:11434 r=192.226.134.67:33450\n",
            "[GIN] 2024/04/07 - 03:43:05 | 200 |      44.911µs |  192.226.134.67 | HEAD     \"/\"\n",
            "[GIN] 2024/04/07 - 03:43:05 | 200 |     599.472µs |  192.226.134.67 | GET      \"/api/tags\"\n",
            "t=2024-04-07T03:43:17+0000 lvl=info msg=\"join connections\" obj=join id=99419a7d1a03 l=127.0.0.1:11434 r=192.226.134.67:33314\n",
            "[GIN] 2024/04/07 - 03:43:17 | 200 |       43.06µs |  192.226.134.67 | HEAD     \"/\"\n",
            "time=2024-04-07T03:43:18.839Z level=INFO source=download.go:136 msg=\"downloading 8934d96d3f08 in 39 100 MB part(s)\"\n",
            "time=2024-04-07T03:43:50.706Z level=INFO source=download.go:136 msg=\"downloading 8c17c2ebb0ea in 1 7.0 KB part(s)\"\n",
            "time=2024-04-07T03:43:52.577Z level=INFO source=download.go:136 msg=\"downloading 7c23fb36d801 in 1 4.8 KB part(s)\"\n",
            "time=2024-04-07T03:43:54.465Z level=INFO source=download.go:136 msg=\"downloading 2e0493f67d0c in 1 59 B part(s)\"\n",
            "time=2024-04-07T03:43:56.358Z level=INFO source=download.go:136 msg=\"downloading fa304d675061 in 1 91 B part(s)\"\n",
            "time=2024-04-07T03:43:58.241Z level=INFO source=download.go:136 msg=\"downloading 42ba7f8a01dd in 1 557 B part(s)\"\n",
            "[GIN] 2024/04/07 - 03:44:16 | 200 | 59.337740602s |  192.226.134.67 | POST     \"/api/pull\"\n",
            "t=2024-04-07T03:44:24+0000 lvl=info msg=\"join connections\" obj=join id=70821d7df259 l=127.0.0.1:11434 r=192.226.134.67:47950\n",
            "[GIN] 2024/04/07 - 03:44:24 | 200 |      73.679µs |  192.226.134.67 | GET      \"/\"\n",
            "t=2024-04-07T03:45:59+0000 lvl=info msg=\"join connections\" obj=join id=3455394c4f72 l=127.0.0.1:11434 r=192.226.134.67:53594\n",
            "[GIN] 2024/04/07 - 03:45:59 | 200 |      66.645µs |  192.226.134.67 | HEAD     \"/\"\n",
            "time=2024-04-07T03:46:00.637Z level=INFO source=download.go:136 msg=\"downloading 970aa74c0a90 in 3 100 MB part(s)\"\n",
            "time=2024-04-07T03:46:04.304Z level=INFO source=download.go:136 msg=\"downloading c71d239df917 in 1 11 KB part(s)\"\n",
            "time=2024-04-07T03:46:06.022Z level=INFO source=download.go:136 msg=\"downloading ce4a164fc046 in 1 17 B part(s)\"\n",
            "time=2024-04-07T03:46:07.762Z level=INFO source=download.go:136 msg=\"downloading 31df23ea7daa in 1 420 B part(s)\"\n",
            "[GIN] 2024/04/07 - 03:46:10 | 200 | 11.202622059s |  192.226.134.67 | POST     \"/api/pull\"\n",
            "t=2024-04-07T03:46:13+0000 lvl=info msg=\"join connections\" obj=join id=6010dbce9f7a l=127.0.0.1:11434 r=192.226.134.67:39532\n",
            "[GIN] 2024/04/07 - 03:46:13 | 200 |      35.237µs |  192.226.134.67 | HEAD     \"/\"\n",
            "[GIN] 2024/04/07 - 03:46:13 | 200 |     839.455µs |  192.226.134.67 | GET      \"/api/tags\"\n",
            "t=2024-04-07T03:46:24+0000 lvl=info msg=\"join connections\" obj=join id=7c97ad14b004 l=127.0.0.1:11434 r=192.226.134.67:39914\n",
            "time=2024-04-07T03:46:24.910Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\n",
            "time=2024-04-07T03:46:24.912Z level=INFO source=cpu_common.go:11 msg=\"CPU has AVX2\"\n",
            "time=2024-04-07T03:46:24.912Z level=INFO source=llm.go:85 msg=\"GPU not available, falling back to CPU\"\n",
            "time=2024-04-07T03:46:24.956Z level=INFO source=dyn_ext_server.go:87 msg=\"Loading Dynamic llm server: /tmp/ollama3521285080/runners/cpu_avx2/libext_server.so\"\n",
            "time=2024-04-07T03:46:24.957Z level=INFO source=dyn_ext_server.go:147 msg=\"Initializing llama server\"\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-8934d96d3f08982e95922b2b7a2c626a1fe873d7c3b06e8e56d7bc0a1fef9246 (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n",
            "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_0:  225 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_0\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW)\n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
            "..................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 2048\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =    70.50 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   164.00 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1060\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "loading library /tmp/ollama3521285080/runners/cpu_avx2/libext_server.so\n",
            "{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":444,\"msg\":\"initializing slots\",\"n_slots\":1,\"tid\":\"138249519441472\",\"timestamp\":1712461589}\n",
            "{\"function\":\"initialize\",\"level\":\"INFO\",\"line\":453,\"msg\":\"new slot\",\"n_ctx_slot\":2048,\"slot_id\":0,\"tid\":\"138249519441472\",\"timestamp\":1712461589}\n",
            "time=2024-04-07T03:46:29.813Z level=INFO source=dyn_ext_server.go:159 msg=\"Starting llama main loop\"\n",
            "{\"function\":\"launch_slot_with_data\",\"level\":\"INFO\",\"line\":826,\"msg\":\"slot is processing task\",\"slot_id\":0,\"task_id\":0,\"tid\":\"138244284110400\",\"timestamp\":1712461589}\n",
            "{\"function\":\"update_slots\",\"ga_i\":0,\"level\":\"INFO\",\"line\":1803,\"msg\":\"slot progression\",\"n_past\":0,\"n_past_se\":0,\"n_prompt_tokens_processed\":25,\"slot_id\":0,\"task_id\":0,\"tid\":\"138244284110400\",\"timestamp\":1712461589}\n",
            "{\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1830,\"msg\":\"kv cache rm [p0, end)\",\"p0\":0,\"slot_id\":0,\"task_id\":0,\"tid\":\"138244284110400\",\"timestamp\":1712461589}\n"
          ]
        }
      ]
    }
  ]
}